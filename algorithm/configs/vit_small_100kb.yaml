run_dir: null
manual_seed: 0
evaluate: false
ray_tune: 0
resume: 0

data_provider:
  dataset: image_folder
  root: null
  resize_scale: 0.875
  color_aug: 0.4
  base_batch_size: 32  # Smaller batch for memory constraint
  n_worker: 4
  image_size: 224
  num_classes: null

run_config:
  # learning rate
  n_epochs: 100  # More epochs for sparse training
  base_lr: 0.0005
  bs256_lr: null
  warmup_epochs: 10
  warmup_lr: 0
  lr_schedule_name: cosine
  # weight decay
  weight_decay: 0.05
  no_wd_keys: ['norm', 'bias', 'pos_embed', 'cls_token']
  # optimizer
  optimizer_name: sgd
  bias_only: 0
  fc_only: 0
  fc_lr10: 0
  # eval sparsely
  eval_per_epochs: 10
  # grid search fine-tuning
  grid_output: null
  grid_ckpt_path: null
  # partial blocks for fp32
  n_block_update: -1

net_config:
  net_name: vit_small_patch16_224
  model_type: fp
  pretrained: false
  cls_head: linear
  dropout: 0.1

backward_config:
  enable_backward_config: 1
  # Conservative sparse update for 100KB memory budget
  n_bias_update: 8  # Update fewer layers
  n_weight_update: null
  vit_layer_types: ['attention', 'head']  # Focus on attention and head only
  weight_update_ratio: 0.25  # Update only 25% of features
  weight_select_criteria: magnitude+
  manual_weight_idx: null
  quantize_gradient: 0
  freeze_fc: 0
  train_scale: 0