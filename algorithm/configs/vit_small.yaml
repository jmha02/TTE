run_dir: null
manual_seed: 0
evaluate: false
ray_tune: 0
resume: 0

data_provider:
  dataset: image_folder
  root: null
  resize_scale: 0.875
  color_aug: 0.4
  base_batch_size: 64
  n_worker: 8
  image_size: 224
  num_classes: null

run_config:
  # learning rate
  n_epochs: 50
  base_lr: 0.001  # Lower LR for ViT
  bs256_lr: null
  warmup_epochs: 5
  warmup_lr: 0
  lr_schedule_name: cosine
  # weight decay
  weight_decay: 0.05  # ViT typically uses weight decay
  no_wd_keys: ['norm', 'bias', 'pos_embed', 'cls_token']
  # optimizer
  optimizer_name: sgd
  bias_only: 0
  fc_only: 0
  fc_lr10: 0
  # eval sparsely
  eval_per_epochs: 10
  # grid search fine-tuning
  grid_output: null
  grid_ckpt_path: null
  # partial blocks for fp32
  n_block_update: -1

net_config:
  net_name: vit_small_patch16_224
  model_type: fp  # Floating point (non-quantized)
  pretrained: false
  cls_head: linear
  dropout: 0.1

backward_config:
  enable_backward_config: 1
  # ViT-specific sparse update configs
  n_bias_update: 15  # Update most layers
  n_weight_update: null
  vit_layer_types: ['attention', 'mlp', 'head']  # Focus on these layer types
  weight_update_ratio: 0.5  # Update 50% of features
  weight_select_criteria: magnitude+
  manual_weight_idx: null
  quantize_gradient: 0
  freeze_fc: 0
  train_scale: 0